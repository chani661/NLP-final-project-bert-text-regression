# -*- coding: utf-8 -*-
"""NLP_final_project_chani factor.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19X3wz50Q9C00mwuG2SfoisM4GgxT2bY2
"""

# imports
import pandas as pd
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import Dataset
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from transformers import BertTokenizer, BertModel, Trainer, TrainingArguments, EarlyStoppingCallback
from transformers.modeling_outputs import SequenceClassifierOutput
import matplotlib.pyplot as plt
import seaborn as sns
from scipy.stats import spearmanr
import warnings
import os
warnings.filterwarnings('ignore')

#Consistency of results
np.random.seed(42)
torch.manual_seed(42)
if torch.cuda.is_available():
    torch.cuda.manual_seed_all(42)

# definding the labels
QUESTION_LABELS = [
    'question_asker_intent_understanding', 'question_body_critical', 'question_conversational',
    'question_expect_short_answer', 'question_fact_seeking', 'question_has_commonly_accepted_answer',
    'question_interestingness_others', 'question_interestingness_self', 'question_multi_intent',
    'question_not_really_a_question', 'question_opinion_seeking', 'question_type_choice',
    'question_type_compare', 'question_type_consequence', 'question_type_definition',
    'question_type_entity', 'question_type_instructions', 'question_type_procedure',
    'question_type_reason_explanation', 'question_type_spelling', 'question_well_written'
]
ANSWER_LABELS = [
    'answer_helpful', 'answer_level_of_information', 'answer_plausible',
    'answer_relevance', 'answer_satisfaction', 'answer_type_instructions',
    'answer_type_procedure', 'answer_type_reason_explanation', 'answer_well_written'
]
ALL_LABELS = QUESTION_LABELS + ANSWER_LABELS
NUM_LABELS = len(ALL_LABELS)

from google.colab import drive
drive.mount('/content/drive')

# getting the data & checking the data
def load_data(file_path):
    df = pd.read_csv(file_path)
    missing = set(ALL_LABELS + ['question_title', 'question_body', 'answer']) - set(df.columns)
    if missing:
        raise ValueError(f"missing value: {missing}")
    return df

# text preprocessing
def advanced_text_preprocessing(df):
    #By [SEP] the model knows where the question ends and the answer begins
    df['processed_text'] = df.apply(
        lambda row: f"{row['question_title']} {row['question_body']} [SEP] {row['answer']}", axis=1
    )
    return df

# creating the dataset
class BERTQADataset(Dataset):
    def __init__(self, texts, labels, tokenizer, max_length=512):
        self.texts = texts
        self.labels = labels
        self.tokenizer = tokenizer
        self.max_length = max_length

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        encoding = self.tokenizer(
            self.texts[idx], truncation=True, padding='max_length',
            max_length=self.max_length, return_tensors='pt'
        )
        return {
            'input_ids': encoding['input_ids'].squeeze(),
            'attention_mask': encoding['attention_mask'].squeeze(), #adjust dim
            'labels': torch.FloatTensor(self.labels[idx])
        }

# the MODEL with molti-target regression
class BERTMultiTargetRegressor(nn.Module):
    def __init__(self, model_name='bert-base-uncased', num_labels=30, dropout_rate=0.3, freeze_bert=False):
        super().__init__()
        self.bert = BertModel.from_pretrained(model_name)
        if freeze_bert:
            for param in self.bert.parameters():
                param.requires_grad = False
        hidden_size = self.bert.config.hidden_size
        self.regressor = nn.Sequential(
            nn.Linear(hidden_size, hidden_size // 2),
            nn.ReLU(),
            nn.Dropout(dropout_rate),
            nn.Linear(hidden_size // 2, hidden_size // 4),
            nn.ReLU(),
            nn.Dropout(dropout_rate),
            nn.Linear(hidden_size // 4, num_labels),
            nn.Sigmoid()
        )

    def forward(self, input_ids, attention_mask, labels=None):
        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)
        cls_output = outputs.last_hidden_state[:, 0]
        logits = self.regressor(cls_output)
        loss = None
        if labels is not None:
            loss_fn = nn.MSELoss()
            loss = loss_fn(logits, labels)
        return SequenceClassifierOutput(loss=loss, logits=logits)

    def unfreeze_bert(self):
        #full training
        for param in self.bert.parameters():
            param.requires_grad = True
        print(" BERT unfreezed – ready for full fine-tuning")

#Tokenizer + Train/Test Split
#actual preperation
def prepare_bert_model_and_data(df, model_name='bert-base-uncased', max_length=512):
    df = advanced_text_preprocessing(df)
    texts = df['processed_text'].tolist()
    labels = df[ALL_LABELS].fillna(0.5).clip(0, 1).astype(np.float32).values
    tokenizer = BertTokenizer.from_pretrained(model_name)
    model = BERTMultiTargetRegressor(model_name=model_name, num_labels=NUM_LABELS, freeze_bert=True)
    X_train, X_test, y_train, y_test = train_test_split(texts, labels, test_size=0.2, random_state=42)
    train_dataset = BERTQADataset(X_train, y_train, tokenizer, max_length)
    test_dataset = BERTQADataset(X_test, y_test, tokenizer, max_length)
    return model, tokenizer, train_dataset, test_dataset, X_test, y_test

# trainer+early stoping
def train_bert_model(
    model, train_dataset, test_dataset,
    output_dir="./bert_regressor",
    learning_rate=2e-5,
    num_train_epochs=20,
    batch_size=8,
    warmup_ratio=0.1,
    patience=2
):
#define training arguments
    training_args = TrainingArguments(
        output_dir=output_dir,
        eval_strategy="epoch", # Changed from evaluation_strategy
        save_strategy="epoch",
        per_device_train_batch_size=batch_size,
        per_device_eval_batch_size=batch_size,
        num_train_epochs=num_train_epochs,
        learning_rate=learning_rate,
        warmup_ratio=warmup_ratio,
        weight_decay=0.01,
        load_best_model_at_end=True,
        metric_for_best_model="r2",
        greater_is_better=True,
        logging_dir=f"{output_dir}/logs",
        logging_steps=10,
        save_total_limit=2,
        seed=42,
        report_to="none"

    )
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=test_dataset,
        compute_metrics=compute_advanced_metrics,
        callbacks=[EarlyStoppingCallback(early_stopping_patience=patience)]
    )
    trainer.train()
    return trainer

# metrics
def compute_advanced_metrics(eval_pred):
    predictions, labels = eval_pred
    predictions = np.clip(predictions, 0, 1)

    mse = mean_squared_error(labels, predictions)
    mae = mean_absolute_error(labels, predictions)
    r2 = r2_score(labels, predictions)

    spearman_all = []
    for i in range(labels.shape[1]):
        coef, _ = spearmanr(labels[:, i], predictions[:, i])
        spearman_all.append(coef)

    avg_spearman = np.nanmean(spearman_all)

    return {
        'mse': mse,
        'mae': mae,
        'r2': r2,
        'avg_spearman': avg_spearman
    }

# eval & graphs
LABEL_NAMES = ALL_LABELS

def evaluate_model(trainer, test_dataset, y_test, label_names=LABEL_NAMES, max_labels_plot=30):
    predictions = trainer.predict(test_dataset)
    y_pred = np.clip(predictions.predictions, 0, 1)

    overall_mse = mean_squared_error(y_test, y_pred)
    overall_mae = mean_absolute_error(y_test, y_pred)
    overall_r2 = r2_score(y_test, y_pred)

    print(f"MAE: {overall_mae:.4f} | MSE: {overall_mse:.4f} | R²: {overall_r2:.4f}")

    mae_per_label = [mean_absolute_error(y_test[:, i], y_pred[:, i]) for i in range(len(label_names))]
    r2_per_label = [r2_score(y_test[:, i], y_pred[:, i]) for i in range(len(label_names))]
    spearman_per_label = [spearmanr(y_test[:, i], y_pred[:, i])[0] for i in range(len(label_names))]

    def print_top_bottom(metric_list, metric_name, higher_is_better=True):
        sorted_indices = np.argsort(metric_list)[::-1] if higher_is_better else np.argsort(metric_list)
        print(f"\n Top 3 labels by {metric_name}:")
        for idx in sorted_indices[:3]:
            print(f" {label_names[idx]}: {metric_list[idx]:.4f}")
        print(f"\n Bottom 3 labels by {metric_name}:")
        for idx in sorted_indices[-3:]:
            print(f" {label_names[idx]}: {metric_list[idx]:.4f}")

    print_top_bottom(r2_per_label, "R²", higher_is_better=True)
    print_top_bottom(mae_per_label, "MAE", higher_is_better=False)
    print_top_bottom(spearman_per_label, "Spearman", higher_is_better=True)

    #MAE
    plt.figure(figsize=(12, 6))
    sns.barplot(x=mae_per_label[:max_labels_plot], y=label_names[:max_labels_plot], palette="Blues_d")
    plt.title(" MAE by label (10 first)")
    plt.xlabel("MAE")
    plt.ylabel("Label")
    plt.tight_layout()
    plt.show()

    #R²
    plt.figure(figsize=(12, 6))
    sns.barplot(x=r2_per_label[:max_labels_plot], y=label_names[:max_labels_plot], palette="Greens_d")
    plt.title(" R² by label (10 first)")
    plt.xlabel("R²")
    plt.ylabel("Label")
    plt.tight_layout()
    plt.show()

    # Spearman
    plt.figure(figsize=(12, 6))
    sns.barplot(x=spearman_per_label[:max_labels_plot], y=label_names[:max_labels_plot], palette="Purples_d")
    plt.title("Spearman correlation by label (first 10)")
    plt.xlabel("Spearman")
    plt.ylabel("Label")
    plt.tight_layout()
    plt.show()

    return {
        'mse': overall_mse,
        'mae': overall_mae,
        'r2': overall_r2,
        'avg_label_mae': np.mean(mae_per_label),
        'avg_label_r2': np.mean(r2_per_label),
        'avg_label_spearman': np.nanmean(spearman_per_label),
        'best_label_r2': np.max(r2_per_label),
        'worst_label_r2': np.min(r2_per_label)
    }

#
df = load_data('/content/drive/MyDrive/NLP/train.csv')
model, tokenizer, train_dataset, test_dataset, X_test, y_test = prepare_bert_model_and_data(df)
trainer = train_bert_model(model, train_dataset, test_dataset)
results = evaluate_model(trainer, test_dataset, y_test)

"""# full train"""

# full train unfreeze
model.unfreeze_bert()

training_args_stage2 = TrainingArguments(
    output_dir="./bert_regressor_stage2",
    eval_strategy="epoch", # Changed from evaluation_strategy
    save_strategy="epoch",
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    num_train_epochs=5,
    learning_rate=2e-5,
    weight_decay=0.01,
    warmup_ratio=0.1,
    load_best_model_at_end=True,
    metric_for_best_model="r2",
    greater_is_better=True,
    logging_dir="./bert_regressor_stage2/logs",
    logging_steps=10,
    save_total_limit=2,
    seed=42,
    report_to="none"
)


trainer_stage2 = Trainer(
    model=model,
    args=training_args_stage2,
    train_dataset=train_dataset,
    eval_dataset=test_dataset,
    compute_metrics=compute_advanced_metrics,
    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]
)


trainer_stage2.train()

results = evaluate_model(trainer_stage2, test_dataset, y_test)

"""# saving model"""

model_save_path = "/content/drive/MyDrive/NLP/bert_model_saved"
os.makedirs(model_save_path, exist_ok=True)

# save weights
torch.save(model.state_dict(), os.path.join(model_save_path, "model_weights.pth"))

# save tokenizer
tokenizer.save_pretrained(model_save_path)

print("tokenizer and weught saved to google drive")

!cp -r /content/bert_regressor /content/drive/MyDrive/NLP/

"""# relaod model, weights & tokenizer"""

# new model object
model = BERTMultiTargetRegressor(model_name='bert-base-uncased', num_labels=NUM_LABELS)
model.load_state_dict(torch.load("/content/drive/MyDrive/NLP/bert model/bert_model_saved/model_weights.pth"))
model.eval()

# laod tokenizer:
tokenizer = BertTokenizer.from_pretrained("/content/drive/MyDrive/NLP/bert model/bert_model_saved")

# laod data
df = load_data('/content/drive/MyDrive/NLP/train.csv')

# text preprocessing
df = advanced_text_preprocessing(df)
texts = df['processed_text'].tolist()
labels = df[ALL_LABELS].fillna(0.5).clip(0, 1).astype(np.float32).values

# test dataset
from sklearn.model_selection import train_test_split
_, X_test, _, y_test = train_test_split(texts, labels, test_size=0.2, random_state=42)

test_dataset = BERTQADataset(X_test, y_test, tokenizer)

from transformers import Trainer, TrainingArguments

eval_args = TrainingArguments(
    output_dir="./tmp_eval",
    per_device_eval_batch_size=8,
    report_to="none"
)

trainer = Trainer(
    model=model,
    args=eval_args,
    compute_metrics=compute_advanced_metrics
)

#  label distrubution in the data
labels_list = [col for col in df.columns if col.startswith("question_") or col.startswith("answer_")]

# calculating the precentage of labels that !=0
label_frequencies = df[labels_list].astype(bool).mean().sort_values()

# visualization
plt.figure(figsize=(12, 8))
sns.barplot(x=label_frequencies.values, y=label_frequencies.index, palette="viridis")
plt.title(" SAMPLE PRECENT FOR EACH LABEL (≠ 0)")
plt.xlabel("SAMPLE PRESENT (0-1)")
plt.ylabel("Label")
plt.grid(True, axis='x', linestyle='--', alpha=0.4)
plt.tight_layout()
plt.show()
label_frequencies.to_frame("Fraction_non_zero").style.format("{:.2%}")

"""# specipic exemple"""

idx = 42

print("כותרת השאלה:\n", df.iloc[idx]['question_title'])
print("\nגוף השאלה:\n", df.iloc[idx]['question_body'])
print("\nהתשובה:\n", df.iloc[idx]['answer'])

from transformers import BertTokenizer

text = f"{df.iloc[idx]['question_title']} {df.iloc[idx]['question_body']} [SEP] {df.iloc[idx]['answer']}"
encoding = tokenizer(
    text,
    truncation=True,
    padding='max_length',
    max_length=512,
    return_tensors='pt'
)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device) # Move model to the device

model.eval()
with torch.no_grad():
    output = model(
        input_ids=encoding['input_ids'].to(device), # Move input tensors to the device
        attention_mask=encoding['attention_mask'].to(device) # Move input tensors to the device
    )
preds = output.logits.cpu().numpy().flatten()

true_vals = df.iloc[idx][ALL_LABELS].values.astype(np.float32)

for label, true_val, pred in zip(ALL_LABELS, true_vals, preds):
    print(f"{label}:  אמת = {true_val:.2f} | תחזית = {pred:.2f}")

import pandas as pd

comparison = pd.DataFrame({
    "Label": ALL_LABELS,
    "True Value": true_vals,
    "Predicted": preds
})
comparison["Error"] = (comparison["Predicted"] - comparison["True Value"]).abs()
comparison.sort_values("Error", ascending=False).head(10)

results = evaluate_model(trainer, test_dataset, y_test)

!pip install torchinfo

from torchinfo import summary
import torch

# Define dummy inputs compatible with the model
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
dummy_inputs = {
    "input_ids": torch.randint(0, 1000, (8, 512)).to(device),
    "attention_mask": torch.ones((8, 512), dtype=torch.int64).to(device)
}

model.to(device) # Ensure model is on the correct device
summary(model, input_data=dummy_inputs, col_names=["input_size", "output_size", "num_params"])

!pip install torchviz

from torchviz import make_dot

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

inputs = {
    "input_ids": torch.randint(0, 100, (1, 512)).to(device),
    "attention_mask": torch.ones((1, 512)).to(device)
}

model.to(device) # Ensure model is on the correct device
output = model(**inputs)
make_dot(output.logits, params=dict(model.named_parameters())).render("model_architecture", format="png")